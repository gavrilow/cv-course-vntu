{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "- [Math for Machine Learning: Multivariate Calculus](https://www.coursera.org/learn/multivariate-calculus-machine-learning)\n",
    "- [Stepic. Нейронные сети](https://stepik.org/course/401)\n",
    "- [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Test](images/nn_brains.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#display results\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "plt.style.use('grayscale')\n",
    "              \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Neural Networks \n",
    "- like a **mathematical function** takes variable IN and gives some BACK\n",
    "\n",
    "\\begin{align}\n",
    "{y} & = f(x)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/nn_in_out_simple.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbf{a}^{(1)} = \\sigma(\\mathbf{w}^{(1)} * \\mathbf{a}^{(0)} + \\mathbf{b}^{(1)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions\n",
    "- a -> **activity**\n",
    "- w -> **weigth**\n",
    "- b -> **bias**\n",
    "- sigma -> **activation function** (associated with brains, neuron activated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why A = WX + B?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/y-mxpb-graph.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Definitions\n",
    "$$ \\mathbf{z}^{(1)} =  \\mathbf{w}^{(1)} * \\mathbf{a}^{(0)} + \\mathbf{b}^{(1)} $$\n",
    "\n",
    "\n",
    "$$ \\mathbf{a}^{(1)} = \\sigma(\\mathbf{z}^{(1)}) $$\n",
    "\n",
    "- sigma => **tanh**, **RELU**, ... [more](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/sigmoid_tanh.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\tanh = \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}} $$\n",
    "\n",
    "$$ \\sigma = \\frac{1}{1 + e^{-z}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/nn_in_out_simple.png)\n",
    "\n",
    "$$ \\mathbf{a}^{(1)} = \\sigma(\\mathbf{w}^{(1)} * \\mathbf{a}^{(0)} + \\mathbf{b}^{(1)}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "σ = np.tanh\n",
    "\n",
    "w1 = 1.3  # weight\n",
    "b1 = -0.1  # bias\n",
    "\n",
    "def a1(a0) :\n",
    "    return σ(w1 * a0 + b1)\n",
    "\n",
    "for x in [0, 1]:\n",
    "    a1(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make more neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/nn_3_neurons.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbf{a}^{(1)} = \\sigma(\\mathbf{w_0} * \\mathbf{a}_0^{(0)} + \\mathbf{w_1} * \\mathbf{a}_1^{(0)} + \\mathbf{b}^{(1)}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/nn_4_neurons.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbf{a}^{(1)} = \\sigma(\\mathbf{w_0} * \\mathbf{a}_0^{(0)} + \\mathbf{w_1} * \\mathbf{a}_1^{(0)} + \\mathbf{w_2} * \\mathbf{a}_2^{(0)} + \\mathbf{b}^{(1)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbf{a}^{(1)} = \\sigma((\\sum_{j=0}^n w_j * a_j^{0}) + b) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplify with dot product \n",
    "- **vector of weights** * **vector of inputs** (sum of element-wise multiplications)\n",
    "\n",
    "$$ \\mathbf{a}^{(1)} = \\sigma(w * a^{0} + b) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/nn_5_neurons.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbf{a}_0^{(1)} = \\sigma(w_0 * a^{0} + b_0) $$\n",
    "\n",
    "- w0 -> vector of **greens**\n",
    "- a0 -> vector of white circles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbf{a}_1^{(1)} = \\sigma(w_1 * a^{0} + b_1) $$\n",
    "\n",
    "- w1 -> vector of **pinks**\n",
    "- a0 -> vector of white circles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplify with dot product\n",
    "- **matrix of vectors of weights** * **vector of inputs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbf{a}^{(1)} = \\sigma(W^{(1)} * a^{(0)} + b^{(1)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy Examples \n",
    "- numpy_basics.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/nn_quiz_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First set up the network.\n",
    "σ = np.tanh\n",
    "\n",
    "W = np.array([[-2, 4, -1],[6, 0, -3]])\n",
    "print(f\"Weights {W.shape}: \\n {W} \\n\")\n",
    "\n",
    "b = np.array([0.1, -2.5])\n",
    "print(f\"Biases {b.shape}: \\n {b} \\n\")\n",
    "\n",
    "# Define our input vector\n",
    "a0 = np.array([0.3, 0.4, 0.1])\n",
    "print(f\"Inputs (a0) {a0.shape}: \\n {a0} \\n\")\n",
    "\n",
    "print(f\"Weights {W.T.shape}: \\n {W.T} \\n\")\n",
    "\n",
    "a1 = σ(a0 @ W.T + b)\n",
    "\n",
    "print(f\"Outputs (a1) {a1.shape}: \\n {a1} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And even more neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/nn_layers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/nn_multi_layer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbf{a}^{(1)} = \\sigma(W^{(1)} * a^{(0)} + b^{(1)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input of current layer = Output of next layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbf{a}^{(2)} = \\sigma(W^{(2)} * a^{(1)} + b^{(2)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Test](images/nn_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbf{a}^{(L)} = \\sigma(W^{(L)} * a^{(L-1)} + b^{(L)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "- Matching **input** to **target output**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz\n",
    "\n",
    "Let's assume we want to train the network to give a NOT function, that is if you input 1 it returns 0, and if you input 0 it returns 1.\n",
    "\n",
    "Which of given **w**, **b** are suitable for network to implement NOT function? \n",
    "\n",
    "$$ f(0) = 1,$$ \n",
    "$$ f(1) = 0 $$\n",
    "\n",
    "\n",
    "- w(1) = 10, b(1) = 0\n",
    "- w(1) = -5, b(1) = 5\n",
    "- w(1) = 3, b(1) = 1\n",
    "- w(1) = -3, b(1) = 0\n",
    "- w(1) = 0, b(1) = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "σ = np.tanh\n",
    "\n",
    "w1 = 1.3  # weight\n",
    "b1 = -0.1  # bias\n",
    "\n",
    "def a1(a0) :\n",
    "    return σ(w1 * a0 + b1)\n",
    "\n",
    "for x in [0, 1]:\n",
    "    a1(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/nn_backprop_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ C = \\sum_i(a_j^{(L)} - y_j)^2 $$\n",
    "\n",
    "- aj - output of neurons\n",
    "- yj - target output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "σ = np.tanh\n",
    "\n",
    "w1 = 1.3  # weight\n",
    "b1 = -0.1  # bias\n",
    "\n",
    "def a1(w1, b1, a0) :\n",
    "    return σ(w1 * a0 + b1)\n",
    "\n",
    "def C(w1, b1, a0, y):\n",
    "    a1_output = a1(w1, b1, a0)\n",
    "    return (a1_output - y)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_w, max_w, num = -10, 10, 100\n",
    "step = (max_w - min_w) / num \n",
    "\n",
    "x = 1  # f(0) = 1 \n",
    "y = 0  # \n",
    "\n",
    "weights = np.linspace(min_w, max_w, num)\n",
    "biases = np.linspace(min_w, max_w, num)\n",
    "\n",
    "w=widgets.FloatSlider(min=min_w,max=max_w,step=step,value=min_w, description=\"weights\")\n",
    "b=widgets.FloatSlider(min=min_w,max=max_w,step=step,value=0, description=\"bias\")\n",
    "@interact(w=w, b=b)\n",
    "def calculate(w, b):\n",
    "    costs = np.array([C(w, b, x, y) for w in weights])\n",
    "    cost = C(w, b, x, y)\n",
    "    \n",
    "    plt.scatter(w, cost, c=\"red\")\n",
    "    plt.plot(weights, costs, c=\"green\")\n",
    "    plt.xlabel(\"weight\")\n",
    "    plt.ylabel(\"cost function\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"weight: {w}, bias: {b}, cost: {cost}\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So we want to Minimize Cost Function\n",
    "- in other words: Find Local Minima of Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geometrical Interpretation\n",
    "- Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/nn_backprop_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Gradient** -> **direction of function growth**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/nn_backprop_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **- Gradient** -> Direction of function decrease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/nn_backprop_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Minimum Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/nn_backprop_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More weights - more difficult it to visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/nn_backprop_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the activation function.\n",
    "sigma = np.tanh\n",
    "\n",
    "# Let's use a random initial weight and bias.\n",
    "W = np.array([[-0.94529712, -0.2667356 , -0.91219181],\n",
    "              [ 2.05529992,  1.21797092,  0.22914497]])\n",
    "b = np.array([ 0.61273249,  1.6422662 ])\n",
    "\n",
    "# define our feed forward function\n",
    "def a1 (a0) :\n",
    "  # Notice the next line is almost the same as previously,\n",
    "  # except we are using matrix multiplication rather than scalar multiplication\n",
    "  # hence the '@' operator, and not the '*' operator.\n",
    "  z = W @ a0 + b\n",
    "  # Everything else is the same though,\n",
    "  return sigma(z)\n",
    "\n",
    "# Next, if a training example is,\n",
    "x = np.array([0.1, 0.5, 0.6])\n",
    "y = np.array([0.25, 0.75])\n",
    "\n",
    "# Then the cost function is,\n",
    "d = a1(x) - y # Vector difference between observed and expected activation\n",
    "C = d @ d # Absolute value squared of the difference.\n",
    "\n",
    "print (C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrices in Python\n",
    "\n",
    "Element wise: when two matrices have the same dimensions, matrix elements in the same position in each matrix are multiplied together\n",
    "In python this uses the '$*$' operator.\n",
    "```python\n",
    "A = B * C\n",
    "```\n",
    "\n",
    "Matrix multiplication: when the number of columns in the first matrix is the same as the number of rows in the second.\n",
    "In python this uses the '$@$' operator\n",
    "```python\n",
    "A = B @ C\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Neural network with 2 hidden layers. There is 1 nodes in the zeroth layer, 6 in the first, 7 in the second, and 2 in the third.](images/big_net.png \"The structure of the network we will consider in this assignment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feed forward\n",
    "\n",
    "In the following cell, we will define functions to set up our neural network.\n",
    "Namely an activation function, $\\sigma(z)$, it's derivative, $\\sigma'(z)$, a function to initialise weights and biases, and a function that calculates each activation of the network using feed-forward.\n",
    "\n",
    "Recall the feed-forward equations,\n",
    "$$ \\mathbf{a}^{(n)} = \\sigma(\\mathbf{z}^{(n)}) $$\n",
    "$$ \\mathbf{z}^{(n)} = \\mathbf{W}^{(n)}\\mathbf{a}^{(n-1)} + \\mathbf{b}^{(n)} $$\n",
    "\n",
    "In this worksheet we will use the *logistic function* as our activation function, rather than the more familiar $\\tanh$.\n",
    "$$ \\sigma(\\mathbf{z}) = \\frac{1}{1 + \\exp(-\\mathbf{z})} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jacobian\n",
    "- partial derivatives of Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- influence of **W**\n",
    "\n",
    "$$ \\mathbf{J}_{\\mathbf{W}^{(3)}} = \\frac{\\partial C}{\\partial \\mathbf{W}^{(3)}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- influence of **B**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbf{J}_{\\mathbf{b}^{(3)}} = \\frac{\\partial C}{\\partial \\mathbf{b}^{(3)}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- where $C$ is the average cost function over the training set. i.e.,\n",
    "$$ C = \\frac{1}{N}\\sum_k C_k $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Chain Rule for **W**\n",
    "$$ \\frac{\\partial C}{\\partial \\mathbf{W}^{(3)}} =\n",
    "   \\frac{\\partial C}{\\partial \\mathbf{a}^{(3)}}\n",
    "   \\frac{\\partial \\mathbf{a}^{(3)}}{\\partial \\mathbf{z}^{(3)}}\n",
    "   \\frac{\\partial \\mathbf{z}^{(3)}}{\\partial \\mathbf{W}^{(3)}}\n",
    "   ,$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Chain Rule for **B**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$ \\frac{\\partial C}{\\partial \\mathbf{b}^{(3)}} =\n",
    "   \\frac{\\partial C}{\\partial \\mathbf{a}^{(3)}}\n",
    "   \\frac{\\partial \\mathbf{a}^{(3)}}{\\partial \\mathbf{z}^{(3)}}\n",
    "   \\frac{\\partial \\mathbf{z}^{(3)}}{\\partial \\mathbf{b}^{(3)}}\n",
    "   .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How C is influenced by **a**\n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial \\mathbf{a}^{(3)}} = 2(\\mathbf{a}^{(3)} - \\mathbf{y}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How A is influenced by **z**\n",
    "\n",
    "$$ \\frac{\\partial \\mathbf{a}^{(3)}}{\\partial \\mathbf{z}^{(3)}} = \\sigma'({z}^{(3)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How Z is influenced by **W**\n",
    "\n",
    "$$ \\frac{\\partial \\mathbf{z}^{(3)}}{\\partial \\mathbf{W}^{(3)}} = \\mathbf{a}^{(2)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How Z is influenced by **B**\n",
    "\n",
    "$$ \\frac{\\partial \\mathbf{z}^{(3)}}{\\partial \\mathbf{b}^{(3)}} = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define our sigma function.\n",
    "sigma = np.tanh\n",
    "sigma_h = lambda z: 1 / np.cosh(z)**2 \n",
    "\n",
    "# Next define the feed-forward equation.\n",
    "def a1 (w1, b1, a0) :\n",
    "    z = w1 * a0 + b1\n",
    "    return sigma(z)\n",
    "\n",
    "# The individual cost function is the square of the difference between\n",
    "# the network output and the training data output.\n",
    "def C (w1, b1, x, y) :\n",
    "    return (a1(w1, b1, x) - y)**2\n",
    "\n",
    "# This function returns the derivative of the cost function with\n",
    "# respect to the weight.\n",
    "def dCdw (w1, b1, x, y) :\n",
    "    z = w1 * x + b1\n",
    "    dCda = 2 * (a1(w1, b1, x) - y) # Derivative of cost with activation\n",
    "    dadz = 1/np.cosh(z)**2 # derivative of activation with weighted sum z\n",
    "    dzdw = x # derivative of weighted sum z with weight\n",
    "    return dCda * dadz * dzdw # Return the chain rule product.\n",
    "\n",
    "# This function returns the derivative of the cost function with\n",
    "# respect to the bias.\n",
    "# It is very similar to the previous function.\n",
    "# You should complete this function.\n",
    "def dCdb (w1, b1, x, y) :\n",
    "    z = w1 * x + b1\n",
    "    dCda = 2 * (a1(w1, b1, x) - y)\n",
    "    dadz = sigma_h(z)\n",
    "    dzdb = 1\n",
    "    return dCda * dadz * dzdb\n",
    "\n",
    "\"\"\"Test your code before submission:\"\"\"\n",
    "# Let's start with an unfit weight and bias.\n",
    "w1 = -5\n",
    "b1 = 5\n",
    "# We can test on a single data point pair of x and y.\n",
    "x = 0\n",
    "y = 1\n",
    "# Output how the cost would change\n",
    "# in proportion to a small change in the bias\n",
    "print(dCdb(w1, b1, x, y))\n",
    "print(dCdw(w1, b1, x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the activation function and its derivative.\n",
    "sigma = lambda z : 1 / (1 + np.exp(-z))\n",
    "d_sigma = lambda z : np.cosh(z/2)**(-2) / 4\n",
    "\n",
    "global W1, W2, W3, b1, b2, b3\n",
    "W1 = None\n",
    "W2 = None\n",
    "W3 = None\n",
    "b1 = None\n",
    "b2 = None\n",
    "b3 = None\n",
    "    \n",
    "# This function initialises the network with it's structure, it also resets any training already done.\n",
    "def reset_network (n1 = 6, n2 = 7, random=np.random) :\n",
    "    global W1, W2, W3, b1, b2, b3\n",
    "    W1 = random.randn(n1, 1) / 2\n",
    "    W2 = random.randn(n2, n1) / 2\n",
    "    W3 = random.randn(2, n2) / 2\n",
    "    b1 = random.randn(n1, 1) / 2\n",
    "    b2 = random.randn(n2, 1) / 2\n",
    "    b3 = random.randn(2, 1) / 2\n",
    "\n",
    "# This function feeds forward each activation to the next layer. It returns all weighted sums and activations.\n",
    "def network_function(a0) :\n",
    "    z1 = W1 @ a0 + b1\n",
    "    a1 = sigma(z1)\n",
    "    z2 = W2 @ a1 + b2\n",
    "    a2 = sigma(z2)\n",
    "    z3 = W3 @ a2 + b3\n",
    "    a3 = sigma(z3)\n",
    "    return a0, z1, a1, z2, a2, z3, a3\n",
    "\n",
    "# This is the cost function of a neural network with respect to a training set.\n",
    "def cost(x, y) :\n",
    "    return np.linalg.norm(network_function(x)[-1] - y)**2 / x.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jacobian for the third layer weights. There is no need to edit this function.\n",
    "def J_W3 (x, y) :\n",
    "    # First get all the activations and weighted sums at each layer of the network.\n",
    "    a0, z1, a1, z2, a2, z3, a3 = network_function(x)\n",
    "    # We'll use the variable J to store parts of our result as we go along, updating it in each line.\n",
    "    # Firstly, we calculate dC/da3, using the expressions above.\n",
    "    J = 2 * (a3 - y)\n",
    "    # Next multiply the result we've calculated by the derivative of sigma, evaluated at z3.\n",
    "    J = J * d_sigma(z3)\n",
    "    # Then we take the dot product (along the axis that holds the training examples)\n",
    "    # with the final partial derivative,\n",
    "    # i.e. dz3/dW3 = a2\n",
    "    # and divide by the number of training examples, for the average over all training examples.\n",
    "    J = J @ a2.T / x.size\n",
    "    # Finally return the result out of the function.\n",
    "    return J\n",
    "\n",
    "# In this function, you will implement the jacobian for the bias.\n",
    "# As you will see from the partial derivatives, only the last partial derivative is different.\n",
    "# The first two partial derivatives are the same as previously.\n",
    "def J_b3 (x, y) :\n",
    "    # As last time, we'll first set up the activations.\n",
    "    a0, z1, a1, z2, a2, z3, a3 = network_function(x)\n",
    "    # Next you should implement the first two partial derivatives of the Jacobian.\n",
    "    J = 2 * (a3 - y)\n",
    "    J = J * d_sigma(z3)\n",
    "    # For the final line, we don't need to multiply by dz3/db3, because that is multiplying by 1.\n",
    "    # We still need to sum over all training examples however.    \n",
    "    J = np.sum(J, axis=1, keepdims=True) / x.size\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll next do the Jacobian for the Layer 2. The partial derivatives for this are,\n",
    "$$ \\frac{\\partial C}{\\partial \\mathbf{W}^{(2)}} =\n",
    "   \\frac{\\partial C}{\\partial \\mathbf{a}^{(3)}}\n",
    "   \\left(\n",
    "   \\frac{\\partial \\mathbf{a}^{(3)}}{\\partial \\mathbf{a}^{(2)}}\n",
    "   \\right)\n",
    "   \\frac{\\partial \\mathbf{a}^{(2)}}{\\partial \\mathbf{z}^{(2)}}\n",
    "   \\frac{\\partial \\mathbf{z}^{(2)}}{\\partial \\mathbf{W}^{(2)}}\n",
    "   ,$$\n",
    "$$ \\frac{\\partial C}{\\partial \\mathbf{b}^{(2)}} =\n",
    "   \\frac{\\partial C}{\\partial \\mathbf{a}^{(3)}}\n",
    "   \\left(\n",
    "   \\frac{\\partial \\mathbf{a}^{(3)}}{\\partial \\mathbf{a}^{(2)}}\n",
    "   \\right)\n",
    "   \\frac{\\partial \\mathbf{a}^{(2)}}{\\partial \\mathbf{z}^{(2)}}\n",
    "   \\frac{\\partial \\mathbf{z}^{(2)}}{\\partial \\mathbf{b}^{(2)}}\n",
    "   .$$\n",
    "This is very similar to the previous layer, with two exceptions:\n",
    "* There is a new partial derivative, in parentheses, $\\frac{\\partial \\mathbf{a}^{(3)}}{\\partial \\mathbf{a}^{(2)}}$\n",
    "* The terms after the parentheses are now one layer lower.\n",
    "\n",
    "Recall the new partial derivative takes the following form,\n",
    "$$ \\frac{\\partial \\mathbf{a}^{(3)}}{\\partial \\mathbf{a}^{(2)}} =\n",
    "   \\frac{\\partial \\mathbf{a}^{(3)}}{\\partial \\mathbf{z}^{(3)}}\n",
    "   \\frac{\\partial \\mathbf{z}^{(3)}}{\\partial \\mathbf{a}^{(2)}} =\n",
    "   \\sigma'(\\mathbf{z}^{(3)})\n",
    "   \\mathbf{W}^{(3)}\n",
    "$$\n",
    "\n",
    "To show how this changes things, we will implement the Jacobian for the weight again and ask you to implement it for the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare this function to J_W3 to see how it changes.\n",
    "# There is no need to edit this function.\n",
    "def J_W2 (x, y) :\n",
    "    #The first two lines are identical to in J_W3.\n",
    "    a0, z1, a1, z2, a2, z3, a3 = network_function(x)    \n",
    "    J = 2 * (a3 - y)\n",
    "    # the next two lines implement da3/da2, first σ' and then W3.\n",
    "    J = J * d_sigma(z3)\n",
    "    J = (J.T @ W3).T\n",
    "    # then the final lines are the same as in J_W3 but with the layer number bumped down.\n",
    "    J = J * d_sigma(z2)\n",
    "    J = J @ a1.T / x.size\n",
    "    return J\n",
    "\n",
    "# As previously, fill in all the incomplete lines.\n",
    "def J_b2 (x, y) :\n",
    "    a0, z1, a1, z2, a2, z3, a3 = network_function(x)\n",
    "    J = 2 * (a3 - y)\n",
    "    J = J * d_sigma(z3)\n",
    "    J = (J.T @ W3).T\n",
    "    J = J * d_sigma(z2)\n",
    "    J = np.sum(J, axis=1, keepdims=True) / x.size\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer 1 is very similar to Layer 2, but with an addition partial derivative term.\n",
    "$$ \\frac{\\partial C}{\\partial \\mathbf{W}^{(1)}} =\n",
    "   \\frac{\\partial C}{\\partial \\mathbf{a}^{(3)}}\n",
    "   \\left(\n",
    "   \\frac{\\partial \\mathbf{a}^{(3)}}{\\partial \\mathbf{a}^{(2)}}\n",
    "   \\frac{\\partial \\mathbf{a}^{(2)}}{\\partial \\mathbf{a}^{(1)}}\n",
    "   \\right)\n",
    "   \\frac{\\partial \\mathbf{a}^{(1)}}{\\partial \\mathbf{z}^{(1)}}\n",
    "   \\frac{\\partial \\mathbf{z}^{(1)}}{\\partial \\mathbf{W}^{(1)}}\n",
    "   ,$$\n",
    "$$ \\frac{\\partial C}{\\partial \\mathbf{b}^{(1)}} =\n",
    "   \\frac{\\partial C}{\\partial \\mathbf{a}^{(3)}}\n",
    "   \\left(\n",
    "   \\frac{\\partial \\mathbf{a}^{(3)}}{\\partial \\mathbf{a}^{(2)}}\n",
    "   \\frac{\\partial \\mathbf{a}^{(2)}}{\\partial \\mathbf{a}^{(1)}}\n",
    "   \\right)\n",
    "   \\frac{\\partial \\mathbf{a}^{(1)}}{\\partial \\mathbf{z}^{(1)}}\n",
    "   \\frac{\\partial \\mathbf{z}^{(1)}}{\\partial \\mathbf{b}^{(1)}}\n",
    "   .$$\n",
    "You should be able to adapt lines from the previous cells to complete **both** the weight and bias Jacobian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J_W1 (x, y) :\n",
    "    a0, z1, a1, z2, a2, z3, a3 = network_function(x)\n",
    "    J = 2 * (a3 - y)\n",
    "    J = J * d_sigma(z3)\n",
    "    J = (J.T @ W3).T\n",
    "    J = J * d_sigma(z2)\n",
    "    J = (J.T @ W2).T\n",
    "    J = J * d_sigma(z1)\n",
    "    J = J @ a0.T / x.size\n",
    "    return J\n",
    "\n",
    "def J_b1 (x, y) :\n",
    "    a0, z1, a1, z2, a2, z3, a3 = network_function(x)\n",
    "    J = 2 * (a3 - y)\n",
    "    J = J * d_sigma(z3)\n",
    "    J = (J.T @ W3).T\n",
    "    J = J * d_sigma(z2)\n",
    "    J = (J.T @ W2).T\n",
    "    J = J * d_sigma(z1)\n",
    "    J = np.sum(J, axis=1, keepdims=True) / x.size\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[ 0., 0.01, 0.02, 0.03,  0.04,  0.05,  0.06,  0.07,  0.08,  0.09,  0.1,   0.11,\n",
    "   0.12,  0.13,  0.14,  0.15,  0.16,  0.17,  0.18,  0.19,  0.2,   0.21,  0.22,  0.23,\n",
    "   0.24,  0.25,  0.26,  0.27,  0.28,  0.29,  0.3,   0.31,  0.32,  0.33,  0.34,  0.35,\n",
    "   0.36,  0.37,  0.38,  0.39,  0.4,   0.41,  0.42,  0.43,  0.44,  0.45,  0.46,  0.47,\n",
    "   0.48,  0.49,  0.5,   0.51,  0.52,  0.53,  0.54,  0.55,  0.56,  0.57,  0.58,  0.59,\n",
    "   0.6,   0.61,  0.62,  0.63,  0.64,  0.65,  0.66,  0.67,  0.68,  0.69,  0.7,   0.71,\n",
    "   0.72,  0.73,  0.74,  0.75,  0.76,  0.77,  0.78,  0.79,  0.8,   0.81,  0.82,  0.83,\n",
    "   0.84,  0.85,  0.86,  0.87,  0.88,  0.89,  0.9,   0.91,  0.92,  0.93,  0.94,  0.95,\n",
    "   0.96,  0.97,  0.98,  0.99,]])\n",
    "print(type(x), x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([[ 0.5,         0.50009902,  0.50078751,  0.50263171,  0.50615226,  0.5118034,\n",
    "   0.51995466,  0.53087547,  0.54472343,  0.56153657,  0.58122992,  0.60359653,\n",
    "   0.62831281,  0.65494819,  0.68297861,  0.7118034,  0.74076505,  0.76917106,\n",
    "   0.7963171,   0.82151087,  0.84409548,  0.86347181,  0.87911897,  0.89061206,\n",
    "   0.89763674,  0.9,         0.89763674,  0.89061206,  0.87911897,  0.86347181,\n",
    "   0.84409548,  0.82151087,  0.7963171,   0.76917106,  0.74076505,  0.7118034,\n",
    "   0.68297861,  0.65494819,  0.62831281,  0.60359653,  0.58122992,  0.56153657,\n",
    "   0.54472343,  0.53087547,  0.51995466,  0.5118034,  0.50615226,  0.50263171,\n",
    "   0.50078751,  0.50009902,  0.5,         0.49990098,  0.49921249,  0.49736829,\n",
    "   0.49384774,  0.4881966,   0.48004534,  0.46912453,  0.45527657,  0.43846343,\n",
    "   0.41877008,  0.39640347,  0.37168719,  0.34505181,  0.31702139,  0.2881966,\n",
    "   0.25923495,  0.23082894,  0.2036829,   0.17848913,  0.15590452,  0.13652819,\n",
    "   0.12088103,  0.10938794,  0.10236326,  0.1,         0.10236326,  0.10938794,\n",
    "   0.12088103,  0.13652819,  0.15590452,  0.17848913,  0.2036829,   0.23082894,\n",
    "   0.25923495,  0.2881966,  0.31702139,  0.34505181,  0.37168719,  0.39640347,\n",
    "   0.41877008,  0.43846343,  0.45527657,  0.46912453,  0.48004534,  0.4881966,\n",
    "   0.49384774,  0.49736829,  0.49921249,  0.49990098],\n",
    " [ 0.625,       0.62701541,  0.63296789,  0.64258068,  0.65540709,  0.67085156,\n",
    "   0.68819755,  0.70664083,  0.72532628,  0.74338643,  0.75997967,  0.77432624,\n",
    "   0.78574006,  0.79365515,  0.79764521,  0.79743558,  0.79290745,  0.78409411,\n",
    "   0.77116996,  0.75443315,  0.73428307,  0.71119423,  0.68568811,  0.65830476,\n",
    "   0.62957574,  0.6,         0.57002377,  0.5400257,   0.51030758,  0.4810911,\n",
    "   0.45252033,  0.42466948,  0.3975551,   0.37115155,  0.34540857,  0.32026952,\n",
    "   0.29568895,  0.27164821,  0.24816805,  0.22531726,  0.20321693,  0.18203995,\n",
    "   0.16200599,  0.14337224,  0.12642077,  0.11144335,  0.0987249,  0.08852676,\n",
    "   0.08107098,  0.07652676,  0.075,       0.07652676,  0.08107098,  0.08852676,\n",
    "   0.0987249,   0.11144335,  0.12642077,  0.14337224,  0.16200599,  0.18203995,\n",
    "   0.20321693,  0.22531726,  0.24816805,  0.27164821,  0.29568895,  0.32026952,\n",
    "   0.34540857,  0.37115155,  0.3975551,  0.42466948,  0.45252033,  0.4810911,\n",
    "   0.51030758,  0.5400257,   0.57002377,  0.6,         0.62957574,  0.65830476,\n",
    "   0.68568811,  0.71119423,  0.73428307,  0.75443315,  0.77116996,  0.78409411,\n",
    "   0.79290745,  0.79743558,  0.79764521,  0.79365515,  0.78574006,  0.77432624,\n",
    "   0.75997967,  0.74338643,  0.72532628,  0.70664083,  0.68819755,  0.67085156,\n",
    "   0.65540709,  0.64258068,  0.63296789,  0.62701541,]])\n",
    "\n",
    "print(type(y), y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green = (0.6862745098039216, 0.8588235294117647, 0.5215686274509804)\n",
    "magentaTrans = (0.9882352941176471, 0.4588235294117647, 0.8588235294117647, 0.1)\n",
    "orange = (0.8549019607843137, 0.6705882352941176, 0.45098039215686275)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(x, y, iterations=10000, aggression=3.5, noise=1) :\n",
    "    global W1, W2, W3, b1, b2, b3\n",
    "    fig,ax = plt.subplots(figsize=(8, 8), dpi= 80)\n",
    "    ax.set_xlim([0,1])\n",
    "    ax.set_ylim([0,1])\n",
    "    ax.set_aspect(1)\n",
    "\n",
    "    xx = np.arange(0,1.01,0.01)\n",
    "    yy = np.arange(0,1.01,0.01)\n",
    "    X, Y = np.meshgrid(xx, yy)\n",
    "    Z = ((X-0.5)**2 + (Y-1)**2)**(1/2) / (1.25)**(1/2)\n",
    "    im = ax.imshow(Z, vmin=0, vmax=1, extent=[0, 1, 1, 0])\n",
    "\n",
    "    ax.plot(y[0],y[1], lw=1.5, color=green);\n",
    "\n",
    "    while iterations>=0 :\n",
    "        j_W1 = J_W1(x, y) * (1 + np.random.randn() * noise)\n",
    "        j_W2 = J_W2(x, y) * (1 + np.random.randn() * noise)\n",
    "        j_W3 = J_W3(x, y) * (1 + np.random.randn() * noise)\n",
    "        j_b1 = J_b1(x, y) * (1 + np.random.randn() * noise)\n",
    "        j_b2 = J_b2(x, y) * (1 + np.random.randn() * noise)\n",
    "        j_b3 = J_b3(x, y) * (1 + np.random.randn() * noise)\n",
    "\n",
    "        W1 = W1 - j_W1 * aggression\n",
    "        W2 = W2 - j_W2 * aggression\n",
    "        W3 = W3 - j_W3 * aggression\n",
    "        b1 = b1 - j_b1 * aggression\n",
    "        b2 = b2 - j_b2 * aggression\n",
    "        b3 = b3 - j_b3 * aggression\n",
    "\n",
    "        if (iterations%10==0) :\n",
    "            # print(x)\n",
    "            nf = network_function(x)[-1]\n",
    "            ax.plot(nf[0],nf[1], lw=2, color=magentaTrans);\n",
    "            # plt.show()\n",
    "        iterations -= 1\n",
    "\n",
    "    nf = network_function(x)[-1]\n",
    "    ax.plot(nf[0],nf[1], lw=2.5, color=orange);\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will iterate through a steepest descent algorithm using the Jacobians you have calculated.\n",
    "The function will plot the training data (in green), and your neural network solutions in pink for each iteration, and orange for the last output.\n",
    "\n",
    "It takes about 50,000 iterations to train this network.\n",
    "We can split this up though - **10,000 iterations should take about a minute to run**.\n",
    "Run the line below as many times as you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training(x, y, iterations=1000, aggression=7, noise=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish, you can change parameters of the steepest descent algorithm (We'll go into more details in future exercises), but you can change how many iterations are plotted, how agressive the step down the Jacobian is, and how much noise to add.\n",
    "\n",
    "You can also edit the parameters of the neural network, i.e. to give it different amounts of neurons in the hidden layers by calling,\n",
    "```python\n",
    "reset_network(n1, n2)\n",
    "```\n",
    "\n",
    "Play around with the parameters, and save your favourite result for the discussion prompt - *I ❤️ backpropagation*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_course",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
