{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "- [Math for Machine Learning: Multivariate Calculus](https://www.coursera.org/learn/multivariate-calculus-machine-learning)\n",
    "- [Stepic. Нейронные сети](https://stepik.org/course/401)\n",
    "- [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Test](images/nn_brains.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "# Standard library\n",
    "import random\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make plots bigger\n",
    "scale = 2\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] *= scale\n",
    "fig_size[1] *= scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/nn_in_out_simple.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- summatory function ?\n",
    "- activation function ?\n",
    "- cost function ?\n",
    "- what are the parameters of neural network?\n",
    "- what is behind training?\n",
    "- aggression? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    "    return numpy.tanh(z)\n",
    "\n",
    "def tanh_prime(z):\n",
    "    return 1. - z * z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return z * (z > 0)\n",
    "\n",
    "def relu_prime(z):\n",
    "    return 1. * (z > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_unstable(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    exp_scores = np.exp(z)\n",
    "    return exp_scores / np.sum(exp_scores)\n",
    "\n",
    "def softmax(z):\n",
    "    exps = np.exp(z - np.max(z))\n",
    "    return exps / np.sum(exps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions (Cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_quadratic(output_activations, y):\n",
    "    \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "    \\partial a for the output activations.\"\"\"\n",
    "    return 0.5 * (output_activations-y)**2\n",
    "\n",
    "def loss_quadratic_derivative(output_activations, y):\n",
    "    \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "    \\partial a for the output activations.\"\"\"\n",
    "    return (output_activations-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_cross_entropy(a, y):\n",
    "    \"\"\"Return the cost associated with an output ``a`` and desired output\n",
    "    ``y``.  Note that np.nan_to_num is used to ensure numerical\n",
    "    stability.  In particular, if both ``a`` and ``y`` have a 1.0\n",
    "    in the same slot, then the expression (1-y)*np.log(1-a)\n",
    "    returns nan.  The np.nan_to_num ensures that that is converted\n",
    "    to the correct value (0.0).\n",
    "    \n",
    "    https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/src/network2.py#L45\n",
    "    \"\"\"\n",
    "    a_ = softmax(a)\n",
    "    return np.sum(np.nan_to_num(-y*np.log(a_)-(1-y)*np.log(1-a_)))\n",
    "\n",
    "def loss_cross_entropy_derivative(z, a, y):\n",
    "    \"\"\"Return the error delta from the output layer.  Note that the\n",
    "    parameter ``z`` is not used by the method.  It is included in\n",
    "    the method's parameters in order to make the interface\n",
    "    consistent with the delta method for other cost classes.\n",
    "    \n",
    "    https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/src/network2.py#L57\n",
    "    \"\"\"\n",
    "    return (a-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offtop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/mvp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network (Advanced Numpy Implementation)\n",
    "\n",
    "- Refactored code from **Neural Networks Basics I**\n",
    "\n",
    "[NeuralNetworksAndDeepLearning](https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/src/network2.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkBasic(object):\n",
    "    \"\"\"\n",
    "    https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/src/network2.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sizes, \n",
    "                 activation=sigmoid,\n",
    "                 activation_derivative=sigmoid_prime,\n",
    "                 cost=loss_quadratic,\n",
    "                 cost_derivative=loss_quadratic_derivative):\n",
    "        \"\"\"The list ``sizes`` contains the number of neurons in the\n",
    "        respective layers of the network.  For example, if the list\n",
    "        was [2, 3, 1] then it would be a three-layer network, with the\n",
    "        first layer containing 2 neurons, the second layer 3 neurons,\n",
    "        and the third layer 1 neuron.  The biases and weights for the\n",
    "        network are initialized randomly, using a Gaussian\n",
    "        distribution with mean 0, and variance 1.  Note that the first\n",
    "        layer is assumed to be an input layer, and by convention we\n",
    "        won't set any biases for those neurons, since biases are only\n",
    "        ever used in computing the outputs from later layers.\"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        \n",
    "        self.activation = activation\n",
    "        self.activation_derivative = activation_derivative\n",
    "        self.cost = cost\n",
    "        self.cost_derivative = cost_derivative\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            # weights (out_neurons x in_neurons)\n",
    "            # biases (out_neurons x 1)\n",
    "            # a (in_neurons x 1) -> vector\n",
    "            # weights @ a -> (out_neurons x 1)\n",
    "            # z = weights @ a + b -> (out_neurons x 1)\n",
    "            # print(w.shape, \"@\", a.shape, \"+\", b.shape)\n",
    "            z = w @ a + b\n",
    "            a = self.activation(z)\n",
    "        return a\n",
    "    \n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``\n",
    "        \n",
    "        Why nabla? https://en.wikipedia.org/wiki/Nabla_symbol\n",
    "        .\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        #  ************** feedforward ********************\n",
    "        a = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        \n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = w @ a + b \n",
    "            a = self.activation(z)\n",
    "            zs.append(z)\n",
    "            activations.append(a)\n",
    "            \n",
    "        # ************* backward pass *********************\n",
    "        \n",
    "        # a (in_neurons, 1)\n",
    "        # y (out_neurons, 1) -> T\n",
    "        # z (out_neurons, 1)\n",
    "        # delta (out_neurons, 1)\n",
    "        # print(\"x \", x.shape, \"a \", activations[-1].shape, \"y \", y.T.shape, \"z \", z.shape)\n",
    "        \n",
    "        # delta (out_neurons, 1)\n",
    "        # dC/da * da/dz\n",
    "        \n",
    "        # updating cost\n",
    "        # self.costs_history.append(self.cost(a, y))\n",
    "        \n",
    "        delta = self.cost_derivative(a, y) * self.activation_derivative(z)\n",
    "        # print(\"delta \", delta.shape)\n",
    "        \n",
    "        # dC/da * da/dz * dz/db (=1)\n",
    "        assert nabla_b[-1].shape == delta.shape, f\"nabla_b {nabla_b[-1].shape}, delta {delta.shape}\"\n",
    "        nabla_b[-1] = delta * 1\n",
    "        \n",
    "        # print(nabla_w[-1].shape, delta.shape, layer_input.shape)\n",
    "        # dC/da * da/dz * dz/dw (=activation of previous layer)\n",
    "        nabla_w[-1] = delta @ activations[-2].T \n",
    "        \n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in range(2, self.num_layers):\n",
    "            \n",
    "            # dzn/da_n-1 * dan/dzn\n",
    "            delta = self.weights[-l+1].T @ delta * self.activation_derivative(zs[-l])\n",
    "            # print(nabla_b[-l].shape, delta.shape)\n",
    "            # print(nabla_w[-l].shape, delta.shape, activations[-l-1].shape)\n",
    "            \n",
    "            # dzn/da_n-1 * dan/dzn * dzn/db\n",
    "            nabla_b[-l] = delta\n",
    "            \n",
    "            # dzn/da_n-1 * dan/dzn * dzn/dw (=activations of previous layer)\n",
    "            nabla_w[-l] = delta @ activations[-l-1].T\n",
    "        return (nabla_b, nabla_w)\n",
    "    \n",
    "    def train(self, training_data, epochs, learning_rate=10e-5, **kwargs):\n",
    "        \"\"\"The ``training_data`` is a list of tuples\n",
    "        ``(x, y)`` representing the training inputs and the desired\n",
    "        outputs.  The other non-optional parameters are\n",
    "        self-explanatory. \"\"\"\n",
    "            \n",
    "        for j in range(epochs):\n",
    "            self.update(training_data, learning_rate)\n",
    "            \n",
    "    \n",
    "    def update(self, training_data, learning_rate=10e-5):\n",
    "        \"\"\"Update the network's weights and biases by applying\n",
    "        gradient descent using backpropagation to a single mini batch.\n",
    "        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n",
    "        is the learning rate.\"\"\"\n",
    "        \n",
    "        # initialize gradients\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        # for each example in training data\n",
    "        # update gradients\n",
    "        for x, y in training_data:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "            \n",
    "        # update weights, biases with calculcated gradients\n",
    "        self.weights = [w - (learning_rate / len(training_data))* nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b - (learning_rate / len(training_data)) * nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **epoch** -> one forward pass and one backward pass of all the training examples\n",
    "- **weights, biases** -> parameters of Neural Network\n",
    "- **learning rate** -> hyperparameter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to be sure that we implemented it correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fx(x):\n",
    "    # return np.sin(w)\n",
    "    return x ** 3 + x ** 2 + x\n",
    "\n",
    "def fx_derivative_analytical(x):\n",
    "    return 3 * x + 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By definition derivative is:\n",
    "\n",
    "$$\\frac{f(x + \\Delta x) - f(x)}{\\Delta x}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fx_derivative_numerical(f, x, eps=10e-6):\n",
    "    return (f(x + eps) - f(x)) / eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 2\n",
    "fx_derivative_analytical(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps=10e-6\n",
    "fx_derivative_numerical(fx, x, eps=eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert fx_derivative_analytical(x) == fx_derivative_numerical(fx, x, eps=eps), \\\n",
    "     f\"{fx_derivative_analytical(x)} != {fx_derivative_numerical(fx, x, eps=eps)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Let's test our functions are working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 2]]).T\n",
    "Y = np.array([[1., 0., 0.]]).T  # one-hot encoded\n",
    "\n",
    "print(f\"X: {X.shape}\")  # vector\n",
    "print(f\"Y: {Y.shape}\")  # vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NetworkBasic([X.shape[0], 3, Y.shape[0]])\n",
    "print(\"Network sizes :\", network.sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed forward\n",
    "y = network.feedforward(X)\n",
    "print(f\"y: {y.shape} \\n\\n {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nabla_b, nabla_w = network.backprop(X, Y)\n",
    "print(f\"nabla_b: {nabla_b} \\n\\n nabla_w: {nabla_w}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Gradient Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By definition partial derivative is:\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x_i} = \\frac{f\\left(x_1,\\ldots,x_{i-1}, x_i + \\Delta x, x_{i+1}, \\ldots, x_d\\right) - f\\left(x_1,\\ldots, x_d\\right)}{\\Delta x}.$$\n",
    "\n",
    "**Note** in definition we use $\\lim _{\\Delta x\\to 0}$! But if we take small enough $\\Delta x$, than the limit wis going to be good enough\n",
    "\n",
    "In other words,\n",
    "- calculate cost function for initial weights, biases\n",
    "- change single weight to some small value\n",
    "- calculcate cost function again\n",
    "- divide the difference between **(1)** and **(3)** by **small value** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A little better way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let calculate instead: \n",
    "\n",
    "$$\\frac{f(x) - f(x + \\Delta x)}{\\Delta x}$$\n",
    "\n",
    "the following  \n",
    "\n",
    "$$\\frac{f(x  + \\Delta x) - f(x - \\Delta x)}{2 \\Delta x}$$\n",
    "\n",
    "in other words let's step to both sides. In practice this method works better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grad_numerically(network, x, y, cost=loss_quadratic, eps=10e-5):\n",
    "    nabla_b = [np.zeros(b.shape, dtype=np.float32) for b in network.biases]\n",
    "    nabla_w = [np.zeros(w.shape, dtype=np.float32) for w in network.weights]\n",
    "        \n",
    "    \n",
    "    # Loop through each layer in network\n",
    "    for l in range(0, network.num_layers - 1):\n",
    "        \n",
    "        # Weights check\n",
    "        weights = network.weights[l]\n",
    "        \n",
    "        # save weights\n",
    "        old_weights = weights.copy()\n",
    "        \n",
    "        w, h = weights.shape[:2][::-1]\n",
    "        for i in range(h):\n",
    "            for j in range(w):\n",
    "                # save previous value\n",
    "                old_value = weights[i, j].copy() \n",
    "                \n",
    "                # calculate f(x + e)\n",
    "                weights[i, j] = old_value + eps\n",
    "                fx_plus_eps = cost(network.feedforward(x), y)\n",
    "                \n",
    "                # calculate f(x - e)\n",
    "                weights[i, j] = old_value - eps\n",
    "                fx_minus_eps = cost(network.feedforward(x), y)\n",
    "                    \n",
    "                # calculcate (f(x + e) - f(x - e)) / (2 * eps)\n",
    "                num_grad = (fx_plus_eps - fx_minus_eps) / (2 * eps)\n",
    "                nabla_w[l][i, j] = np.sum((fx_plus_eps - fx_minus_eps)) / (2 * eps)\n",
    "                    \n",
    "                # restore previous value\n",
    "                weights[i, j] = old_value\n",
    "        \n",
    "        # check we don't broke out network\n",
    "        assert np.allclose(weights, old_weights), \"We broke network\"\n",
    "        \n",
    "        \n",
    "        # Biases check\n",
    "        biases = network.biases[l] \n",
    "        \n",
    "        # save biases\n",
    "        old_biases = biases.copy()\n",
    "        n = biases.shape[0]\n",
    "        \n",
    "        # Loop through all biases\n",
    "        for i in range(n):\n",
    "            # save previous value\n",
    "            old_value = biases[i].copy() \n",
    "            \n",
    "            # calculate f(x + e)\n",
    "            biases[i] = old_value + eps\n",
    "            fx_plus_eps = cost(network.feedforward(x), y)\n",
    "                \n",
    "            # calculate f(x - e)\n",
    "            biases[i] = old_value - eps\n",
    "            fx_minus_eps = cost(network.feedforward(x), y)\n",
    "                \n",
    "            # calculcate (f(x + e) - f(x - e)) / (2 * eps)\n",
    "            nabla_b[l][i] = np.sum((fx_plus_eps - fx_minus_eps)) / (2 * eps)\n",
    "                \n",
    "            biases[i] = old_value # old_weight\n",
    "            \n",
    "        # check we don't broke out network\n",
    "        assert np.allclose(biases, old_biases), \"We broke network\"\n",
    "    \n",
    "        \n",
    "    return nabla_b, nabla_w  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 10e-5\n",
    "nabla_b_numerical, nabla_w_numerical = compute_grad_numerically(network, X, Y, eps=eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nabla_b_, nabla_b_numerical_ in zip(nabla_b, nabla_b_numerical):\n",
    "    assert (np.allclose(nabla_b_, nabla_b_numerical_, atol=eps))\n",
    "    \n",
    "for nabla_w_, nabla_w_numerical_ in zip(nabla_w, nabla_w_numerical):\n",
    "    assert (np.allclose(nabla_w_, nabla_w_numerical_, atol=eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"nabla_b: {nabla_b} \\n\\n nabla_b_numerical: {nabla_b_numerical_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"nabla_w: {nabla_w} \\n\\n nabla_w_numerical: {nabla_w_numerical_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To remember"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **relative error > 1e-2**: usually means the gradient is probably **wrong**\n",
    "\n",
    "- **1e-2 > relative error > 1e-4** should make you feel **uncomfortable**\n",
    "\n",
    "- **1e-4 > relative error** is **usually okay** for objectives with kinks. But if there are no kinks (e.g. use of tanh nonlinearities and softmax), then 1e-4 is too high.\n",
    "\n",
    "- **1e-7 and less** you should **be happy**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful links\n",
    "- [Introduction into Tensorflow with Deep Learning](https://www.udacity.com/course/intro-to-tensorflow-for-deep-learning--ud187)\n",
    "- [What the idea behind Machine Learning](https://youtu.be/UxKbUwj5hmU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IRIS Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![flower](https://archive.ics.uci.edu/ml/assets/MLimages/Large53.jpg)\n",
    "\n",
    "As a motivating example, consider the [iris flower dataset](https://archive.ics.uci.edu/ml/datasets/iris). The dataset consists\n",
    "of 150 data points where each data point is a feature vector $\\boldsymbol x \\in \\mathbb{R}^4$ describing the attribute of a flower in the dataset, the four dimensions represent \n",
    "\n",
    "1. sepal length in cm \n",
    "2. sepal width in cm \n",
    "3. petal length in cm \n",
    "4. petal width in cm \n",
    "\n",
    "\n",
    "and the corresponding target $y \\in \\mathbb{Z}$ describes the class of the flower. It uses the integers $0$, $1$ and $2$ to represent the 3 classes of flowers in this dataset.\n",
    "\n",
    "0. Iris Setosa\n",
    "1. Iris Versicolour \n",
    "2. Iris Virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors, datasets\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('data shape is {}'.format(iris.data.shape))\n",
    "print('class shape is {}'.format(iris.target.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data[:, :2] # use first two version for simplicity\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000',  '#00FF00', '#0000FF'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "for i, iris_class in enumerate(['Iris Setosa', 'Iris Versicolour', 'Iris Virginica']):\n",
    "    idx = y==i\n",
    "    ax.scatter(X[idx,0], X[idx,1], \n",
    "               c=cmap_bold.colors[i], edgecolor='k', \n",
    "               s=20, label=iris_class);\n",
    "ax.set(xlabel='sepal length (cm)', ylabel='sepal width (cm)')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ? What the problem with **Y**\n",
    "- what last layer of Neural Network returns? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot-Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y: np.ndarray) -> np.ndarray:\n",
    "    num_y = y.shape[0]\n",
    "    n_classes = np.max(y) + 1    \n",
    "    one_hot_encoded = np.zeros((num_y, n_classes))\n",
    "    #empty one-hot matrix\n",
    "    one_hot_encoded[np.arange(num_y), y] = 1\n",
    "    return one_hot_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = one_hot_encode(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(y.shape[0]):\n",
    "    print(f\"Before: {y[i]}, After: {Y[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('data shape is {}'.format(X.shape))\n",
    "print('class shape is {}'.format(Y.shape))\n",
    "assert X.shape[0] == Y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[:, :, np.newaxis]  # add new dimension\n",
    "Y = Y[:, :, np.newaxis]  # add new dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('data shape is {}'.format(X.shape))\n",
    "print('class shape is {}'.format(Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [(X[i], Y[i]) for i in range(X.shape[0])]  # list of tuple X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_data[0][0].shape, training_data[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NetworkBasic([X.shape[1], Y.shape[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "total_cost = []\n",
    "for i in range(epochs):\n",
    "    network.update(training_data, learning_rate=0.01)\n",
    "    \n",
    "    total_cost.append(np.mean([network.cost(network.feedforward(x), y) \\\n",
    "                               for x, y in training_data]))\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(f\"Epoch {i} complete. Loss: {total_cost[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ? What should we expect from cost after each epoch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(total_costs)), total_costs)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"cost\")\n",
    "plt.title(\"Cost by epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ? What the problem with previous graph ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(nnetwork, data):\n",
    "    \"\"\"Return the number of test inputs for which the neural\n",
    "    network outputs the correct result. Note that the neural\n",
    "    network's output is assumed to be the index of whichever\n",
    "    neuron in the final layer has the highest activation.\"\"\"\n",
    "    results = [(np.argmax(nnetwork.feedforward(x)), np.argmax(y))\n",
    "                        for (x, y) in data]\n",
    "    return sum(int(x == y) for (x, y) in results) / len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NetworkBasic([X.shape[1], Y.shape[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "accuracy = []\n",
    "for i in range(epochs):\n",
    "    network.update(training_data, learning_rate=0.1)\n",
    "    \n",
    "    accuracy.append(evaluate(network, training_data))\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(f\"Epoch {i} complete. Accuracy: {accuracy[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(accuracy)), accuracy)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.title(\"Training Accuracy by epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ? What the problem with previous graph ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Test, Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(training_data, train_rate=0.8):\n",
    "    random.shuffle(training_data)\n",
    "    n_train = int(len(training_data) * train_rate)\n",
    "    return training_data[:n_train], training_data[n_train:]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = split(training_data, train_rate=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ([np.argmax(y) for x, y in test_data])\n",
    "print(f\"Classes: {classes}, \\nNum classes: {len(classes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NetworkBasic([X.shape[1], Y.shape[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "accuracy = []\n",
    "for i in range(epochs):\n",
    "    network.update(train_data, learning_rate=0.1)\n",
    "    \n",
    "    accuracy.append(evaluate(network, test_data))\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(f\"Epoch {i} complete. Accuracy: {accuracy[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(accuracy)), accuracy)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.title(\"Test Accuracy by epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Test, Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/train_valid_test.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Under/Over fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NetworkBasic([X.shape[1], Y.shape[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "test_accuracy = []\n",
    "train_accuracy = []\n",
    "for i in range(epochs):\n",
    "    network.update(train_data, learning_rate=0.1)\n",
    "    \n",
    "    test_accuracy.append(evaluate(network, test_data))\n",
    "    train_accuracy.append(evaluate(network, train_data))\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(f\"Epoch {i} complete. Test Accuracy: {test_accuracy[-1]}, Train Accuracy: {train_accuracy[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_plot, = plt.plot(range(len(test_accuracy)), test_accuracy)\n",
    "train_plot, = plt.plot(range(len(train_accuracy)), train_accuracy)\n",
    "plt.legend([test_plot, train_plot], ['test', 'train'])\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.title(\"Accuracy by epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/sgd.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkSGD(NetworkBasic):\n",
    "    \n",
    "    def __init__(self, sizes, \n",
    "                 activation=sigmoid,\n",
    "                 activation_derivative=sigmoid_prime,\n",
    "                 cost=loss_quadratic,\n",
    "                 cost_derivative=loss_quadratic_derivative):\n",
    "        \n",
    "        NetworkBasic.__init__(self, \n",
    "                              sizes,\n",
    "                              activation,\n",
    "                              activation_derivative,\n",
    "                              cost, \n",
    "                              cost_derivative)\n",
    "        \n",
    "    def train(self, training_data, epochs, \n",
    "            batch_size, learning_rate=10e-5):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic\n",
    "        gradient descent.  The ``training_data`` is a list of tuples\n",
    "        ``(x, y)`` representing the training inputs and the desired\n",
    "        outputs.  The other non-optional parameters are\n",
    "        self-explanatory.\"\"\"\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]]\n",
    "            for mini_batch in mini_batches:\n",
    "                self._update(mini_batch, learning_rate)              \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NetworkSGD([X.shape[1], Y.shape[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "test_accuracy = []\n",
    "train_accuracy = []\n",
    "for i in range(epochs):\n",
    "    network.update(train_data, learning_rate=0.1)\n",
    "    \n",
    "    test_accuracy.append(evaluate(network, test_data))\n",
    "    train_accuracy.append(evaluate(network, train_data))\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(f\"Epoch {i} complete. Test Accuracy: {test_accuracy[-1]}, Train Accuracy: {train_accuracy[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ? SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ?  Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/learningrates_clear.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, w in enumerate(network.weights): \n",
    "    print(f\"Layer {i}\")\n",
    "    sns.heatmap(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Hinton diagrams](https://matplotlib.org/gallery/specialty_plots/hinton_demo.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinton(matrix, max_weight=None, ax=None):\n",
    "    \"\"\"Draw Hinton diagram for visualizing a weight matrix.\"\"\"\n",
    "    ax = ax if ax is not None else plt.gca()\n",
    "\n",
    "    if not max_weight:\n",
    "        max_weight = 2 ** np.ceil(np.log(np.abs(matrix).max()) / np.log(2))\n",
    "\n",
    "    ax.patch.set_facecolor('gray')\n",
    "    ax.set_aspect('equal', 'box')\n",
    "    ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "    ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "    for (x, y), w in np.ndenumerate(matrix):\n",
    "        color = 'white' if w > 0 else 'black'\n",
    "        size = np.sqrt(np.abs(w) / max_weight)\n",
    "        rect = plt.Rectangle([x - size / 2, y - size / 2], size, size,\n",
    "                             facecolor=color, edgecolor=color)\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    ax.autoscale_view()\n",
    "    ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, w in enumerate(network.weights): \n",
    "    print(f\"Layer {i}\")\n",
    "    hinton(w.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Assignment](https://stepik.org/lesson/21780/step/1?unit=5198)\n",
    "- Just Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_course",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
